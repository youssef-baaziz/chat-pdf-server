from langchain.callbacks.base import BaseCallbackHandler
from flask_socketio import emit

class CustomStreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        super().__init__()  # Call the parent class initializer
        self.emit_func = emit  # Function to emit socket messages
        self.current_response = ""  # Store the full response progressively
        self.is_streaming_response = False  # Track when streaming starts
        self.i = 0  # Track token index if needed (currently unused)

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called whenever a new token is generated by the LLM."""
        if self.i == 2:
            # Emit the new token via socket.io and print it
            self.emit_func('streaming_chunk', {'content': token})    
            print(token, end='', flush=True)
            
            self.current_response += token
        else:
            return

    def get_full_response(self) -> str:
        """Return the full response generated by the LLM."""
        return self.current_response
    
    def on_llm_start(self, serialized: dict, prompts: list, **kwargs) -> None:
        """Called when the LLM starts generating tokens."""
        print("\nModel is starting to generate a response...")
        self.i = self.i + 1
        self.is_streaming_response = True  # Set flag indicating the response is being streamed

    def on_llm_end(self, response, **kwargs) -> None:
        """Called when the LLM finishes generating tokens."""
        print("\nModel has finished generating the response.")
        self.is_streaming_response = False  # Reset flag once response is complete

    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """Called if there is an error during token generation."""
        print(f"An error occurred: {error}")
        self.is_streaming_response = False  # Reset flag in case of error
